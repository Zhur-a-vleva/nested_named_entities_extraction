{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1Bk-soJW4XoLAsMLjlGPjhzSirE64cAm6","authorship_tag":"ABX9TyPOzU/dg8IZlbYuK78KaDET"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports"],"metadata":{"id":"UFxuWCY54gNV"}},{"cell_type":"code","execution_count":56,"metadata":{"id":"yNPAQwVm-XSa","executionInfo":{"status":"ok","timestamp":1714244326093,"user_tz":-180,"elapsed":12,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}}},"outputs":[],"source":["import json\n","import re\n","from collections import Counter"]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"LE7YR_ex4iyE"}},{"cell_type":"code","source":["def read_file(file_path):\n","    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","        data = [json.loads(line) for line in f]\n","    return data\n","\n","# create pairs of words and their corresponding ners\n","def count_ners(train):\n","    ners = {}\n","    for listing in train:\n","        for start, end, ner in listing[\"ners\"]:\n","            word = ''.join(listing[\"sentences\"][start:end+1])\n","            ners.setdefault(word, []).append(ner)\n","    return ners\n","\n","# define the most common ner\n","def most_common_ner(ners):\n","    for key in ners.keys():\n","        ners[key] = Counter(ners[key]).most_common(1)[0]\n","    return ners\n","\n","def tokenize(text):\n","    return re.findall(r\"\\w+|\\n\", text)"],"metadata":{"id":"X4tHU5pRhbHI","executionInfo":{"status":"ok","timestamp":1714244326869,"user_tz":-180,"elapsed":35,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["# Predict ners, using dictionary with pairs"],"metadata":{"id":"JIb81MWs4xki"}},{"cell_type":"code","source":["train = read_file(\"/content/drive/MyDrive/Colab Notebooks/Assignment 3/test/train.jsonl\")\n","ners = most_common_ner(count_ners(train))"],"metadata":{"id":"_R97ymSKhbpU","executionInfo":{"status":"ok","timestamp":1714244326871,"user_tz":-180,"elapsed":35,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["text = read_file(\"/content/drive/MyDrive/Colab Notebooks/Assignment 3/test/test.jsonl\")\n","\n","# process each text entry to identify NERs and their positions within the text\n","# this is done by tokenizing the text and checking if each token is in the NERs dictionary\n","# the result is a list of lists, where each inner list contains tuples of start and end positions and the NER type.\n","tuples = [\n","    [(data[\"senences\"].find(token), data[\"senences\"].find(token) + len(token) - 1, ners[token][0])\n","     for token in tokenize(data[\"senences\"]) if token in ners]\n","    for data in text\n","]\n","\n","predictions = []\n","\n","# iterate through each tuples list to merge entities\n","# if the current entity is of a different type or not adjacent to the previous entity, it is added as a new entry\n","# otherwise, the end position of the last entry is updated to include the current entity\n","for prediction in tuples:\n","    merged_prediction = []\n","    for start, end, ner in prediction:\n","        if not merged_prediction or ner != merged_prediction[-1][2] or end + 2 != merged_prediction[-1][1]:\n","            merged_prediction.append([start, end, ner])\n","        else:\n","            merged_prediction[-1][1] = end\n","    predictions.append(merged_prediction)\n","\n","result = [{\"ners\": predictions[i], \"id\": text[i][\"id\"]} for i in range(len(text))]"],"metadata":{"id":"OF-CdFiBhyoe","executionInfo":{"status":"ok","timestamp":1714244326872,"user_tz":-180,"elapsed":34,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/Colab Notebooks/Assignment 3/output_dict/test.jsonl', 'w') as outfile:\n","    for entry in result:\n","        json.dump(entry, outfile)\n","        outfile.write('\\n')\n","\n","# for convenient folder structure in colab\n","!zip -r /content/drive/MyDrive/Colab\\ Notebooks/Assignment\\ 3/output_dict/test.zip /content/drive/MyDrive/Colab\\ Notebooks/Assignment\\ 3/output_dict/test.jsonl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"181o5UFAh2uZ","executionInfo":{"status":"ok","timestamp":1714244326875,"user_tz":-180,"elapsed":34,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}},"outputId":"41b49acb-42ea-466f-8b06-243e83e88353"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["updating: content/drive/MyDrive/Colab Notebooks/Assignment 3/output_dict/test.jsonl (deflated 79%)\n"]}]},{"cell_type":"code","source":["# for submission\n","!zip test test.jsonl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5bTuvh1h62Q","executionInfo":{"status":"ok","timestamp":1714244327431,"user_tz":-180,"elapsed":573,"user":{"displayName":"Dasha Zhuravleva","userId":"09802255901674395530"}},"outputId":"e3e3f723-596e-4d91-81be-4d583bc4537f"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["updating: test.jsonl (deflated 79%)\n"]}]}]}